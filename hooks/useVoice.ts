// --------------------------- useVoice.ts ---------------------------
import { Audio } from 'expo-av';
import * as FileSystem from 'expo-file-system';
import { useCallback, useRef, useState } from 'react';
import { PermissionsAndroid, Platform } from 'react-native';
import { textToSpeech, transcribeAudio } from '../utils/gcpServices';

interface UseVoiceSessionProps {
  onTranscriptReceived?: (transcript: string) => void;
  // YENÄ°: Seslendirme durumunu bildirmek iÃ§in daha genel bir callback
  onSpeechPlaybackStatusUpdate?: (status: { isPlaying: boolean }) => void;
  onSpeechStarted?: () => void;
  onSpeechEnded?: () => void;
  onSoundLevelChange?: (level: number) => void;
  therapistId?: string;
}

export const useVoiceSession = ({
  onTranscriptReceived,
  onSpeechPlaybackStatusUpdate, // YENÄ°
  onSpeechStarted,
  onSpeechEnded,
  onSoundLevelChange,
  therapistId = 'therapist1',
}: UseVoiceSessionProps = {}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const recording = useRef<Audio.Recording | null>(null);
  const sound = useRef<Audio.Sound | null>(null);
  const levelTimer = useRef<ReturnType<typeof setInterval> | null>(null);

  /** Platforma gÃ¶re izin diyaloÄŸu */
  const requestPermission = async () => {
    if (Platform.OS === 'android') {
      const granted = await PermissionsAndroid.request(
        PermissionsAndroid.PERMISSIONS.RECORD_AUDIO,
        {
          title: 'Mikrofon Ä°zni',
          message: 'Sesli terapi iÃ§in mikrofona eriÅŸim gerekiyor',
          buttonPositive: 'Tamam',
        }
      );
      return granted === PermissionsAndroid.RESULTS.GRANTED;
    } else if (Platform.OS === 'ios') {
      const { status } = await Audio.requestPermissionsAsync();
      return status === 'granted';
    }
    return true;
  };

  const startRecording = useCallback(async () => {
    console.log("ðŸŽ¤ ATTEMPTING TO START RECORDING...");
    if (isRecording) {
      console.log("   -> Already recording, returning.");
      return;
    }
    const ok = await requestPermission();
    if (!ok) {
      console.log("   -> Permission denied, returning.");
      return;
    }

    try {
      // Genel ses modu ayarÄ± - hem kayÄ±t hem oynatÄ±m iÃ§in optimize edilmiÅŸ
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
        staysActiveInBackground: true,
        shouldDuckAndroid: true,
        playThroughEarpieceAndroid: false // Android'de hoparlÃ¶rden Ã§alsÄ±n
      });
      console.log("   -> Audio mode set.");

      // ----> GÃœNCEL KAYIT SEÃ‡ENEKLERÄ° <----
      const customRecordingOptions = {
        android: {
          extension: '.wav',
          outputFormat: Audio.AndroidOutputFormat.DEFAULT,
          audioEncoder: Audio.AndroidAudioEncoder.DEFAULT,
          sampleRate: 16000,
          numberOfChannels: 1,
        },
        ios: {
          extension: '.wav',
          audioQuality: Audio.IOSAudioQuality.MAX,
          sampleRate: 16000,
          numberOfChannels: 1,
          bitRate: 128000, // <-- EKSÄ°K OLAN BUYDU. ZAFÄ°YET GÄ°DERÄ°LDÄ°.
          outputFormat: Audio.IOSOutputFormat.LINEARPCM,
          linearPCMBitDepth: 16,
          linearPCMIsBigEndian: false,
          linearPCMIsFloat: false,
        },
        web: {},
      };
      const { recording: rec } = await Audio.Recording.createAsync(
        customRecordingOptions
      );
      console.log("   -> Recording object created.");

      recording.current = rec;
      setIsRecording(true); // <-- Bu state'in gÃ¼ncellenmesi Ã‡OK Ã–NEMLÄ°
      onSpeechStarted?.();
      console.log("âœ… RECORDING STARTED SUCCESSFULLY.");

      // Ses seviyesi Ã¶lÃ§Ã¼mÃ¼
      levelTimer.current = setInterval(async () => {
        if (!recording.current) return;
        const status = await recording.current.getStatusAsync();
        if (status.isRecording && status.metering)
          onSoundLevelChange?.(status.metering);
      }, 120);
    } catch (err) {
      console.error('ðŸ”´ FAILED TO START RECORDING:', err);
    }
  }, [isRecording, onSoundLevelChange, onSpeechStarted]);

  const stopRecording = useCallback(async () => {
    console.log("ðŸ›‘ ATTEMPTING TO STOP RECORDING...");
    if (!recording.current) {
      console.log("   -> No recording object found, returning.");
      return;
    }
    if (levelTimer.current) {
      clearInterval(levelTimer.current);
      levelTimer.current = null;
    }
    setIsRecording(false); // <-- Bu state'in gÃ¼ncellenmesi Ã‡OK Ã–NEMLÄ°
    setIsProcessing(true);

    try {
      await recording.current.stopAndUnloadAsync();
      const uri = recording.current.getURI();
      console.log("   -> Recording stopped and unloaded. URI:", uri);
      if (uri) {
        const info = await FileSystem.getInfoAsync(uri);
        // info.size sadece exists:true ise vardÄ±r
        const fileSize = info.exists ? info.size : 0;
        const fileExt = uri.split('.').pop();
        // console.log('[VOICE] KayÄ±t URI:', uri, 'Boyut:', fileSize, 'Format:', fileExt, 'exists:', info.exists);
      }
      recording.current = null;
      onSpeechEnded?.();

      if (uri) {
        try {
          console.log('ðŸŽ¯ [VOICE-HOOK] Ses tanÄ±ma baÅŸlatÄ±lÄ±yor...', { uri, fileExists: true });
          const text = await transcribeAudio(uri);
          console.log('ðŸ“ [VOICE-HOOK] Ses tanÄ±ma tamamlandÄ±:', {
            text,
            length: text?.length,
            isEmpty: !text || text.trim().length === 0
          });
          onTranscriptReceived?.(text);
        } catch (err) {
          console.error('âŒ [VOICE-HOOK] Ses tanÄ±ma hatasÄ±:', err);
          onTranscriptReceived?.('');
        }
      } else {
        console.log('âš ï¸ [VOICE-HOOK] Ses dosyasÄ± URI bulunamadÄ±');
        onTranscriptReceived?.('');
      }
    } catch (err) {
      console.error('ðŸ”´ FAILED TO STOP RECORDING:', err);
    } finally {
      setIsProcessing(false);
      console.log("âœ… PROCESSING FINISHED.");
    }
  }, [onSpeechEnded, onTranscriptReceived]);

  // speakText fonksiyonundan yaÅŸ parametresini kaldÄ±r
  const speakText = useCallback(async (text: string, therapistIdArg?: string) => {
    try {
      // Ses Ã§almadan Ã¶nce hoparlÃ¶r moduna geÃ§
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: false, // Sadece oynatÄ±m iÃ§in
        playsInSilentModeIOS: true,
        staysActiveInBackground: true,
        shouldDuckAndroid: false,
        playThroughEarpieceAndroid: false // HoparlÃ¶rden Ã§alsÄ±n
      });

      // therapistId'yi gcpServices'e iletiyoruz (artÄ±k userAge yok)
      const url = await textToSpeech(text, therapistIdArg || therapistId);
      const { sound: s } = await Audio.Sound.createAsync(
        { uri: url },
        { shouldPlay: true, volume: 1.0, isMuted: false },
        (status) => {
          if (status.isLoaded) {
            console.log('ðŸ”Š [VOICE-HOOK] Playback status update:', { isPlaying: status.isPlaying, didJustFinish: status.didJustFinish });
            onSpeechPlaybackStatusUpdate?.({ isPlaying: status.isPlaying });
            if (status.didJustFinish) {
              s.unloadAsync();
              onSpeechPlaybackStatusUpdate?.({ isPlaying: false });
            }
          }
        }
      );
      sound.current = s;
    } catch (err) {
      console.warn('Ses Ã§alÄ±namadÄ±:', err);
      onSpeechPlaybackStatusUpdate?.({ isPlaying: false });
    }
  }, [therapistId, onSpeechPlaybackStatusUpdate]);

  const cleanup = useCallback(async () => {
    if (levelTimer.current) clearInterval(levelTimer.current);
    levelTimer.current = null;
    if (recording.current) await recording.current.stopAndUnloadAsync();
    if (sound.current) await sound.current.unloadAsync();
  }, []);

  return {
    isRecording,
    isProcessing,
    startRecording,
    stopRecording,
    speakText,
    cleanup,
  };
};